{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2888fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from cerebras.cloud.sdk import DefaultAioHttpClient\n",
    "from cerebras.cloud.sdk import AsyncCerebras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2d78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b7a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5d6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CEREBRAS_API_KEY=config.get(\"CEREBRAS_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2bc9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def main() -> None:\n",
    "    async with AsyncCerebras(\n",
    "        api_key=CEREBRAS_API_KEY,\n",
    "        http_client=DefaultAioHttpClient(),\n",
    "    ) as client:\n",
    "        chat_completion = await client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Why is fast inference important?\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3.1-8b\",\n",
    "        )\n",
    "    print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97331ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionResponse(id='chatcmpl-68b264db-0b59-462d-b5b6-4016992dcc9c', choices=[ChatCompletionResponseChoice(finish_reason='stop', index=0, message=ChatCompletionResponseChoiceMessage(role='assistant', content='Fast inference is crucial in various applications where timely decisions and responses are essential. Here are some reasons why fast inference is important:\\n\\n1. **Real-time Decision Making**: In applications such as autonomous vehicles, robots, and video surveillance, fast inference enables real-time decision making. This is critical for ensuring the safety of people, avoiding accidents, and maintaining the stability of the system.\\n\\n2. **Improved User Experience**: Fast inference results in quicker responses and faster rendering of output, which improves the overall user experience in applications like computer vision, natural language processing, and recommender systems.\\n\\n3. **Efficient Resource Utilization**: With fast inference, resources like CPU, GPU, and memory usage can be optimized, allowing applications to perform more tasks simultaneously without experiencing significant performance drops.\\n\\n4. **Scalability and Cost-Effectiveness**: Faster inference models can process more data and requests in a shorter amount of time, making it possible to scale up the application without significant increases in infrastructure costs.\\n\\n5. **Business Continuity**: In critical applications such as healthcare and finance, fast inference helps ensure business continuity by providing timely and accurate results even during peak usage periods.\\n\\n6. **Competitive Advantage**: Fast inference models can provide a competitive advantage by enabling companies to quickly analyze and respond to changes in the market, customer behavior, and business needs.\\n\\n7. **Reduced Wait Times**: Fast inference results in shorter wait times for users, which is particularly important in applications like customer service chatbots, language translation systems, and content personalization.\\n\\nIn summary, fast inference is essential for real-time decision making, improved user experience, efficient resource utilization, scalability, business continuity, competitive advantage, and reduced wait times.', reasoning=None, tool_calls=None), logprobs=None)], created=1759265878, model='llama3.1-8b', object='chat.completion', system_fingerprint='fp_fd2238778934236570f3', time_info=ChatCompletionResponseTimeInfo(completion_time=0.151668041, prompt_time=0.002071687, queue_time=0.000308272, total_time=0.1696152687072754, created=1759265878), usage=ChatCompletionResponseUsage(completion_tokens=341, prompt_tokens=41, prompt_tokens_details=ChatCompletionResponseUsagePromptTokensDetails(cached_tokens=0), total_tokens=382), service_tier=None)\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9850a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
