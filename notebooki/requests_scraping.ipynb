{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import os\n",
    "import json\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_webpage_with_image_positions(webpage_url, save_dir):\n",
    "    \"\"\"\n",
    "    Scrape a webpage, download images, and create a text representation\n",
    "    with markers indicating image positions.\n",
    "    \n",
    "    Args:\n",
    "        webpage_url (str): URL of the webpage to scrape\n",
    "        save_dir (str): Directory to save images and output\n",
    "        \n",
    "    Returns:\n",
    "        dict: Content with text and image metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send a GET request to the webpage\n",
    "        response = requests.get(webpage_url)\n",
    "        response.raise_for_status()  # Check for HTTP issues\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Create directory if not exists\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        # Initialize content collection\n",
    "        content = {\n",
    "            \"text\": \"\",\n",
    "            \"images\": []\n",
    "        }\n",
    "        \n",
    "        # Process the body of the document\n",
    "        body = soup.body or soup\n",
    "        process_element(body, content, webpage_url, save_dir)\n",
    "        \n",
    "        # Save the content structure to a JSON file\n",
    "        output_path = os.path.join(save_dir, \"content.json\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(content, f, indent=2)\n",
    "            \n",
    "        # Save the text with image markers to a separate file\n",
    "        text_path = os.path.join(save_dir, \"content.txt\")\n",
    "        with open(text_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content[\"text\"])\n",
    "        \n",
    "        print(f\"Scraped content saved to {output_path}\")\n",
    "        print(f\"Text with image markers saved to {text_path}\")\n",
    "        return content\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to scrape webpage: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_element(element, content, base_url, save_dir):\n",
    "    \"\"\"\n",
    "    Recursively process an HTML element and its children,\n",
    "    extracting text and images in the order they appear.\n",
    "    \n",
    "    Args:\n",
    "        element: BeautifulSoup element\n",
    "        content: Dictionary to store content\n",
    "        base_url: Base URL for resolving relative URLs\n",
    "        save_dir: Directory to save images\n",
    "    \"\"\"\n",
    "    # Skip script, style, and other non-content elements\n",
    "    if element.name in ['script', 'style', 'meta', 'link', 'head']:\n",
    "        return\n",
    "    \n",
    "    # Process all children of this element\n",
    "    for child in element.children:\n",
    "        if isinstance(child, NavigableString):\n",
    "            # Handle text content\n",
    "            text = child.strip()\n",
    "            if text:\n",
    "                content[\"text\"] += text + \" \"\n",
    "        elif child.name == 'img':\n",
    "            # Handle image element\n",
    "            process_image(child, content, base_url, save_dir)\n",
    "        elif child.name in ['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            # Process block elements and add appropriate spacing\n",
    "            process_element(child, content, base_url, save_dir)\n",
    "            content[\"text\"] += \"\\n\\n\"\n",
    "        elif child.name == 'br':\n",
    "            # Handle line breaks\n",
    "            content[\"text\"] += \"\\n\"\n",
    "        elif child.name == 'a':\n",
    "            # For links, get the text but also track that it was a link\n",
    "            link_text = child.get_text().strip()\n",
    "            if link_text:\n",
    "                content[\"text\"] += link_text + \" \"\n",
    "            # Recursively process any nested elements in the link\n",
    "            process_element(child, content, base_url, save_dir)\n",
    "        else:\n",
    "            # Process other elements recursively\n",
    "            process_element(child, content, base_url, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img_tag, content, base_url, save_dir):\n",
    "    \"\"\"\n",
    "    Process an image element, download it, and insert a marker in the text.\n",
    "    \n",
    "    Args:\n",
    "        img_tag: BeautifulSoup img element\n",
    "        content: Dictionary to store content\n",
    "        base_url: Base URL for resolving relative URLs\n",
    "        save_dir: Directory to save images\n",
    "    \"\"\"\n",
    "    img_url = img_tag.get('src')\n",
    "    if not img_url:\n",
    "        return\n",
    "    \n",
    "    # Handle relative URLs\n",
    "    if not img_url.startswith(('http:', 'https:')):\n",
    "        img_url = requests.compat.urljoin(base_url, img_url)\n",
    "    \n",
    "    # Generate a unique ID for this image\n",
    "    img_id = str(uuid.uuid4())[:8]\n",
    "    \n",
    "    # Get original filename or generate one\n",
    "    original_filename = img_url.split('/')[-1]\n",
    "    filename = f\"{img_id}_{original_filename}\"\n",
    "    local_path = os.path.join(save_dir, filename)\n",
    "    \n",
    "    # Get image alt text if available\n",
    "    alt_text = img_tag.get('alt', '')\n",
    "    \n",
    "    # Create a marker for this image position\n",
    "    marker = f\"[IMAGE:{img_id}]\"\n",
    "    \n",
    "    # Insert the marker at the current position in the text\n",
    "    content[\"text\"] += marker + \" \"\n",
    "    \n",
    "    # Record image metadata\n",
    "    image_info = {\n",
    "        \"id\": img_id,\n",
    "        \"url\": img_url,\n",
    "        \"local_path\": local_path,\n",
    "        \"alt_text\": alt_text,\n",
    "        \"marker\": marker\n",
    "    }\n",
    "    content[\"images\"].append(image_info)\n",
    "    \n",
    "    # Download the image\n",
    "    try:\n",
    "        response = requests.get(img_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(local_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"Downloaded: {local_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {img_url}: {e}\")\n",
    "        image_info[\"download_error\"] = str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_document(content_path, output_path, image_format=\"html\"):\n",
    "    \"\"\"\n",
    "    Reconstruct a document from the content.json file,\n",
    "    replacing image markers with actual images.\n",
    "    \n",
    "    Args:\n",
    "        content_path (str): Path to the content.json file\n",
    "        output_path (str): Path to save the reconstructed document\n",
    "        image_format (str): Format for image inclusion (\"html\" or \"markdown\")\n",
    "    \"\"\"\n",
    "    with open(content_path, 'r', encoding='utf-8') as f:\n",
    "        content = json.load(f)\n",
    "    \n",
    "    text = content[\"text\"]\n",
    "    images = content[\"images\"]\n",
    "    \n",
    "    # Replace each image marker with the appropriate image tag\n",
    "    for img in images:\n",
    "        marker = img[\"marker\"]\n",
    "        if image_format == \"html\":\n",
    "            img_tag = f'<img src=\"{img[\"local_path\"]}\" alt=\"{img[\"alt_text\"]}\" />'\n",
    "        else:  # markdown\n",
    "            img_tag = f'![{img[\"alt_text\"]}]({img[\"local_path\"]})'\n",
    "        \n",
    "        text = text.replace(marker, img_tag)\n",
    "    \n",
    "    # Save the reconstructed document\n",
    "    if image_format == \"html\":\n",
    "        text = f\"<html><body>{text}</body></html>\"\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    print(f\"Reconstructed document saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = \"https://zw01f.github.io/malware%20analysis/auto-color/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = \"https://gootloader.wordpress.com/2025/03/31/gootloader-returns-malware-hidden-in-google-ads-for-legal-documents/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/bartek/Kod/PD/praca_dyplomowa/notebooki/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/3120c4c2_logo.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/51aab4d1_avatar.jpg\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/f57202cb_VT.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/9cd1735e_flow.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/9ec1eaad_str_dec.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/9f8735e8_installation.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/ff6d72fd_lock_file.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/a2751c8e_fork.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/74844779_enc_config.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/b7ca948d_config_decryption.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/2d87406a_checksum.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/79f78ac8_send_message.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/dee533e8_recieved.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/8deefc3d_command_1.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/148ee0ea_command_15.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/db0315b9_command_400.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/57c87f45_dir_or_file.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/7aa0895b_reverse_shell.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/c70a0588_proxy.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/3b53671e_hook_example.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/127518d1_run_auto.png\n",
      "Downloaded: /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/112ba46f_hide_network.png\n",
      "Scraped content saved to /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/content.json\n",
      "Text with image markers saved to /home/bartek/Kod/PD/praca_dyplomowa/notebooki/images/content.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'scraped_content/content.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Reconstruct the document (optional)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content:\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mreconstruct_document\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreconstructed.html\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhtml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m, in \u001b[0;36mreconstruct_document\u001b[0;34m(content_path, output_path, image_format)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreconstruct_document\u001b[39m(content_path, output_path, image_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Reconstruct a document from the content.json file,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    replacing image markers with actual images.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m        image_format (str): Format for image inclusion (\"html\" or \"markdown\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m         content \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     14\u001b[0m     text \u001b[38;5;241m=\u001b[39m content[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Kod/PD/pd_env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scraped_content/content.json'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = url1\n",
    "    save_directory = \"scraped_content\"\n",
    "    \n",
    "    # Scrape the webpage with image positions\n",
    "    content = scrape_webpage_with_image_positions(url, path)\n",
    "    \n",
    "    # Reconstruct the document (optional)\n",
    "    if content:\n",
    "        reconstruct_document(\n",
    "            os.path.join(save_directory, \"content.json\"),\n",
    "            os.path.join(save_directory, \"reconstructed.html\"),\n",
    "            image_format=\"html\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
